## tensorflow学习笔记(一)

[TOC]



##### uint8取值范围

uint8对应int8

int8取值范围-128---127，2的7次方

uint8是无符号整数，0--255，0到2的八次方



##### reshape()函数

可以通过reshape()函数改变数组的维度

```python
import numpy as np
int_array = np_array(for i in range(64))
print(int_array)
array.reshape(8,8)
print(int_array)
```



##### 逻辑回归基本模型

使用sigmiod函数进行激活的作用就是:

- 将线性函数变为非线性的
- 将输出值变为一个概率值, 即0-1之间的值

即sigmoid函数的作用就是对线性函数做一个**归一化**

> **归一化和标准化**的异同. 
>
> （1）区别. 归一化是将样本的特征值转换到同一量纲下把数据映射到 [0,1]或者 [-1, 1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。. 标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，转换为标准正态分布，和整体样本分布相关，每个样本点都能对标准化产生影响。. 
>
> （2）相同. 它们的相同点在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量X按照比例压缩再进行平移。



##### 逻辑回归中的损失函数

*均方损失函数 适用于线性回归模型

因为均方误差采用的是sigmoid激活函数, 在求偏导进行梯度下降时, 在值域趋近于1时, sigmoid的值不会发生太大的变化, 导致sigmoid的导数无限趋近于0, 参数的梯度不会发生太大的变化, 导致可能出现梯度消失的情况, 使得参数更新非常缓慢

而交叉熵损失函数采用softmax激活函数, 不受激活函数导数的影响, 只受预测结果和真实结果的影响, 当误差大的时候，权重更新快；当误差小的时候，权重更新慢。

激活函数也可以使用sigmoid函数, 但是效果不好

softmax激活函数适用于多分类问题

[参考资料]([逻辑回归为何不用平方差损失函数 - 简书 (jianshu.com)](https://www.jianshu.com/p/6a7d3f26f003))



##### onehot编码(独热编码)

当minist识别时, 使用独热编码, 假如要识别1, 只要预测的不是1, 那么就认为误差是一样的

而**欧氏距离**则会认为预测3会比预测8的误差小, 因为3和1的距离比8和1的距离近

但是我们想要的是唯一确定的结果, 独热编码将每一个值进行0和1的编码, 当要预测误差时, 就进行两个值之间的独热编码的相减, 然后取平均值, 以此来预测误差

[参考资料]([数据预处理之onehot编码 - 简书 (jianshu.com)](https://www.jianshu.com/p/38f9f426e246))



##### 交叉熵

交叉熵刻画的是两个概率分布之间的距离

用onehot编码中的0 1值乘以模型预测分布中的概率得出交叉熵损失



**交叉熵关注标签类别**

因为利用了onehot编码只有0和1, 及有关和无关, 所以当为0时, 就直接计算为0, 相当于忽略该数据, 

只关心概率对不对, 离得近的就好, 离得远的就不好

考虑的较少, 能够符合某些情况的要求, 比如minist手写数字识别



##### 模型训练步骤

1. 准备数据
2. 定义模型(数学公式)
3. 定义损失loss()
4. 定义优化器(例如梯度下降)
5. 迭代训练



##### 数据集分配

训练集: [ 6 / 7 ]

验证集: 用在训练之中, 用来判断某一阶段的训练结果怎么样, 用于参数的调整  [1]

测试集: 用于训练之后, 验证模型的训练结果 [3 / 2]

